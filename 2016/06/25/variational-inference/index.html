<!DOCTYPE html><html lang="zh-CN"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="description"><meta name="viewport" content="width=device-width, initial-scale=1"><title>Zhengwen Li</title><link rel="short icon" href="/favicon.ico"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/fonts/iconfont/iconfont.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.5.0/css/font-awesome.min.css"><link rel="stylesheet" href="//fonts.googleapis.com/css?family=Source+Sans+Pro:400,600|Roboto Mono"><!-- baidu analytics--><script type="text/javascript">var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?9e0cbea7d3319c6c94c71dfb93c151b8";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><!-- google analytics--><script type="text/javascript">(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-74273646-1', 'auto');
ga('send', 'pageview');</script></head><body><div id="main"><header><a href="/." class="logo">Zhengwen Li</a><ul class="nav"><li class="nav-link"><a href="/" target="_self">首页</a></li><li class="nav-link"><a href="/archives/" target="_self">归档</a></li><li class="nav-link"><a href="/tags/" target="_self">标签</a></li><li class="nav-link"><a href="/about/" target="_self">关于</a></li></ul></header><section id="container"><article class="post"><h1 class="post-title">Variational Inference 变分推断</h1><span class="post-time">2016年6月25日</span><div class="post-content"><p>在机器学习的模型中，很容易就会遇到需要求解后验分布$p(\theta \mid x)$的情况，举例来说，假设有一堆观测变量$\mathbf x = \left\lbrace x_0, x_1, …, x_n \right\rbrace$，我们已经知道这些观测变量的产生是由隐藏因子$\mathbf z = \left\lbrace z_0, z_1, …, z_k \right\rbrace$ 来决定的，然后我们希望机器学习的模型能够根据来推断出$z$的分布，也就是求得$p(z \mid x)$，这样就使得我们有能力对未来的观测数据进行预测。<br>这是一个很明显的贝叶斯后验概率估计的问题，<br>$$p(\theta\mid x) = \frac{p(x\mid\theta)p(\theta)}{p(x)}$$其中$p(x) = \sum_\theta p(x\mid\theta)p(\theta)$，我们称之为evidence。<br>情况看起来很简单，我们只需要运用一下贝叶斯公式，求解一下就可以了，但是，在有些情况下，后验是很难求的，原因是evidence不好求解，在$\theta$是连续或者虽然是离散，但是变量的取值空间确是无限大的情况下，evidence的计算量就会变得很大，甚至无法计算。  </p>
<h4 id="混合高斯">混合高斯</h4><p>假设观测数据$\mathbf x = \left\lbrace x_0, x_1, …, x_n \right\rbrace$，是由以下过程产生的：</p>
<ol>
<li>数据是从由$k$高斯混合而成的模型产生出来的，每个高斯模型有它自己的参数$(u_k, \sigma_k)$，混合的比例因子是$\mathbf{\pi} = \left\lbrace \pi_0, …, \pi_k \right\rbrace$</li>
<li>$u_k ~ \mathcal{N} (0, \sigma^2)$</li>
<li>对于第i样本，先从k个高斯里选择一个高斯：$z_j ~ \mathbf{\pi}$</li>
<li>根据对应的高斯，采样出观测值：$x_i ~ \mathcal{N} (u_{z_j}, \sigma_{z_j}^2)$  </li>
</ol>
<p>我们可以使用最大化后验估计的方法来进行参数推导，后验估计的计算为：<br>$$p(u_{1:K}, z_{1:N}) = \frac{\prod_{k=1}^Kp(u_k)\prod_{i=1}^Np(z_i)p(x_i \mid z_i, u_{1:K})}{\int_{u_{1:K}}\sum_{z_{1:N}}{\prod_{k=1}^Kp(u_k)\prod_{i=1}^Np(z_i)p(x_i \mid z_i, u_{1:K})}}$$ 明显地，这个式子的evidence是一个计算量是很大的分布。一般这样的分布称为difficult-to-compute分布。<br>difficult-to-compute分布是很难进行精确求解的，我们只能退而进行近似求解。对于分布进行近似求解，在贝叶斯统计里，具有非常重要的意义，因为贝叶斯统计把对隐藏因子的求解都归结为求这些隐藏因子的后验概率。目前来讲，主要有两种类型的方法来近似这些difficult-to-compute分布，一种是采样的方法，比如说MCMC和importance sampling，采样算法的核心思想在于，希望通过一定的手段，比如Markov Chain，采样出很多的样本，使得这些样本的分布，尽量是和要求的分布式一致的，然后再通过简单的统计，来得到对要求分布的估计。另一种方法就是optimization的方法，就是本文要讲的variational inference 变分推断算法。  </p>
<h4 id="变分推断的算法框架">变分推断的算法框架</h4><p>变分推断的背后的思想是，首先选择一个关于隐藏因子$z$的假设空间$q(z \mid \nu)$，然后通过寻找空间中的一个分布$q(z_i \mid \nu_i)$，使得$q(z_i \mid \nu_i)$尽量和$p(z \mid x)$相等，最后，使用$q(z_i \mid \nu_i)$来进行参数推导。<br>对于两个分布的相似性，使用KL散度来度量$$KL(q(z \mid \nu) \mid {p(z \mid x)}) = E_q\left[log\frac{q(z \mid \nu)}{p(z \mid x)}\right]$$ KL散度越小，说明两个分布就越相似，我们可以通过最小化KL散度来得到最逼近$p(z \mid x)$的分布，但是，其实我们也是不能够直接对KL进行最小化求解，因为直接的求解还是会涉及到前面需要计算evidence的问题。虽然我们不能够直接地最小化KL散度，但是我们确是能够间接地最小化它，这涉及到一个概念<strong>evidence lower bound (ELBO))</strong></p>
<h5 id="Evidence_Lower_Bound">Evidence Lower Bound</h5><p>$$<br>\begin{align}<br>KL(q(z \mid \nu) \mid {p(z \mid x)}) &amp; = E_q\left[log\frac{q(z \mid \nu)}{p(z \mid x)}\right] \<br> &amp; = E_qlogq(z\mid \nu) - E_qlogp(x, z) + logp(x) \<br> &amp; = E_qlogp(x) + \left[E_qlogq(z\mid\nu) - E_qlogp(x, z)\right]<br>\end{align}<br>$$<br>其中$ELBO = \left[E_qlogq(z\mid\nu) - E_qlogp(x, z)\right]$<br>可以看出，因为$E_qlog(p(x))$是一个常量，所以最大化$ELBO$，就相当于最小化KL散度。<br>剩下的问题就是如何最大化$ELBO$的问题了，下次再写好了，好累。。。  </p>
<h5 id="未完待续…">未完待续…</h5></div></article><div class="tags"><a href="/tag/Machine-Learning-variational-inference/">Machine Learning; variational inference</a></div><div class="paginator"><a href="/2015/09/20/Topic-Model系列之HDP/" class="next"><span>下一页</span><i class="fa fa-chevron-right"></i></a></div><section id="comments"><div id="disqus_thread"></div></section><script type="text/javascript">(function() {
var d = document, s = d.createElement('script');

s.src = '//zwli.disqus.com/embed.js';

s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script></section><footer><div class="copyright"><p><span class="heart"><i class="fa fa-heart"></i></span><span class="author">Zhengwen Li</span></p><p class="theme">Theme by <a href="https://github.com/ahonn/hexo-theme-even"> Even</a></p></div><label id="back2top"><i class="fa fa-chevron-up"></i></label><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script></footer></div></body><script src="/js/back2top.js"></script></html>